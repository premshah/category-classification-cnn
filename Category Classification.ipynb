{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output is in \"output.xlsx\" file\n",
    "\n",
    "This Kernel(Jupyter Notebook) is building and training a convolutional neural network for text classification. We are classifying products into categories based on their Brand and description(title)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading training data and creating two list of training_categories(CategoryName) and training_texts(BrandName + Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TRAINING_FILE = \"Training_Data_Assessment.xlsx\"\n",
    "training_sheets = pd.ExcelFile(TRAINING_FILE)\n",
    "training_data_df = training_sheets.parse(\"training_data\")\n",
    "training_categories = []\n",
    "training_texts = []\n",
    "for index, row in training_data_df.iterrows():\n",
    "    text = row[\"BrandName\"].lower() + \" \" + row[\"Title\"].lower()\n",
    "    category = row[\"CategoryName\"].lower()\n",
    "    training_texts.append(text)\n",
    "    training_categories.append(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tokenizer class to tokenize texts of training data using Keras text processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class WordTokenizer:\n",
    "    \"\"\"Class which tokenizes words\n",
    "    Attributes:\n",
    "        max_sequence_length (int): Maximum sequence length for embedding\n",
    "        tokenizer (Tokenizer): Keras Tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_sequence_length=200):\n",
    "        \"\"\"Create tokenizer\n",
    "        Args:\n",
    "            max_sequence_length (int): Maximum sequence length for texts\n",
    "        \"\"\"\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.tokenizer = None\n",
    "        \n",
    "    def train(self, texts, max_nb_words=15000):\n",
    "        \"\"\"Takes a list of texts, fits a tokenizer to them, and create the embedding matrix.\n",
    "        Args:\n",
    "            texts (list(str)): List of texts\n",
    "            max_nb_words: Maximum number of words indexed\n",
    "        \"\"\"\n",
    "        # Tokenize\n",
    "        print('Training tokenizer...')\n",
    "        self.tokenizer = Tokenizer(num_words=max_nb_words,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'')\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        print('Found %s unique tokens.' % len(self.tokenizer.word_index))\n",
    "\n",
    "    def tokenize(self, texts):\n",
    "        \"\"\"Takes a list of texts and tokenizes them.\n",
    "        Args:\n",
    "            texts (list(str)): List of texts\n",
    "        Returns:\n",
    "            np.array: 2D numpy array (len(texts), self.max_sequence_length)\n",
    "        \"\"\"\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        data = pad_sequences(sequences, maxlen=self.max_sequence_length)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer...\n",
      "Found 12325 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordTokenizer()\n",
    "tokenizer.train(training_texts)\n",
    "tokenized_data = tokenizer.tokenize(training_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried different models and different hyper parameters and finally choose a 2D CNN. \n",
    "\n",
    "I tried using word embeddings with pretrained data(weights) as well and I found that performance of model increases when used with correct word embedding pretrained model. I tried Glove and Word2Vec and found that Glove had no impact on performance because mostly it did not had any ecommerce specific words like accessories, etc but Word2Vec had many of them thus I finally decided to use Word2Vec model and use it to pass as weight matrix while doing embedding. I really feel and suggest that we should have ecommerce specific word embeddings like we have Word2Vec and Glove for News relted data, it can really enhance performance of model\n",
    "\n",
    "I tried many models but finally had to choose between two of them, one was 1D CNN with LSTM layer at top of it and other is 2d CNN and I found that performance of 2D CNN is better than 1D CNN with LSTM layer. And I think reason for this better performance is 2D CNN and figure out more hidden details and can form more hidden connection which 1D CNN with LSTM can not. I am able to develop a model with 2d CNN having 94% traing accuracy and 82% validation accuracy, I am pretty sure this model will work will and give good accuracy for test data. I think having almost 63K records helped but if thier was more data then performance of model will also increase.\n",
    "\n",
    "I also found that using 0.5 dropout fraction and batch size of 32 and number of epochs as 35 seems a good set of hyper parameters. I fugured out all hyper parameters after several runs of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets load the pretrain Word2Vec model from Google https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "It might take time since it contains contains 300-dimensional vectors for 3 million words and phrases\n",
    "\n",
    "do,\n",
    "\n",
    "cd ..\n",
    "\n",
    "unzip GoogleNews-vectors-negative300.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A class for Product Classifier\n",
    "\n",
    "commented code are another embedding method and 1D CNN model with LSTM on top, I have left them deliberately so it can be checked if needed. \n",
    "\n",
    "This class helps in compiling and training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prems\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Product classifier class\"\"\"\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import gensim\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Conv1D, MaxPooling1D, Embedding, LSTM, Conv2D, MaxPooling2D, concatenate\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers.core import Reshape\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from keras import regularizers\n",
    "\n",
    "class ProductClassifier(object):\n",
    "    \"\"\"Class which classifies products based on various inputs\n",
    "       Attributes:\n",
    "           prefix (str): Model files prefix\n",
    "           model (keras.model): Keras model\n",
    "           category_map (dict(str, int)): Map between category names and indices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Load in model and category map\n",
    "        \"\"\"\n",
    "        self.model = None\n",
    "        self.category_map = {}\n",
    "\n",
    "    def index_categories(self, categories):\n",
    "        \"\"\"Take a list of possibly duplicate categories and create an index list\n",
    "        Args:\n",
    "            categories (list(str)): List of categories\n",
    "        Returns:\n",
    "            list(int): List of indices\n",
    "        \"\"\"\n",
    "        print('Indexing categories...')\n",
    "        indices = []\n",
    "        for category in categories:\n",
    "            if not (category in self.category_map):\n",
    "                self.category_map[category] = len(self.category_map)\n",
    "            indices.append(self.category_map[category])\n",
    "        print('Found %s unique categories.' % len(self.category_map))\n",
    "        return indices\n",
    "\n",
    "    def classify(self, data):\n",
    "        \"\"\"Classify by products by text\n",
    "        Args:\n",
    "            data (np.array): 2D array representing descriptions of the product and/or product title\n",
    "        Returns:\n",
    "            list(dict(str, float)): List of dictionaries of product categories with associated confidence\n",
    "        \"\"\"\n",
    "        prediction = self.model.predict(data)\n",
    "        all_category_probs = []\n",
    "        for i in range(prediction.shape[0]):\n",
    "            category_probs = {}\n",
    "            for category in self.category_map:\n",
    "                category_probs[category] = prediction[i,self.category_map[category]]\n",
    "            all_category_probs.append(category_probs)\n",
    "        return all_category_probs\n",
    "\n",
    "    def get_labels(self, categories):\n",
    "        \"\"\"Create labels from a list of categories\n",
    "        Args:\n",
    "            categories (list(str)): A list of product categories\n",
    "        Returns:\n",
    "            (list(int)): List of indices\n",
    "        \"\"\"\n",
    "        indexed_categories = self.index_categories(categories)\n",
    "        labels = to_categorical(np.asarray(indexed_categories))\n",
    "        return labels\n",
    "\n",
    "    def compile(self, tokenizer, embedding_dim=300, dropout_fraction=0.5, kernal_size=5, n_filters=128):\n",
    "        \"\"\"Compile network model for classifier\n",
    "        Args:\n",
    "            embedding_dim (int): Size of embedding vector\n",
    "            tokenizer (WordTokenizer): Object used to tokenize orginal texts\n",
    "            dropout_fraction (float): Fraction of randomly zeroed weights in dropout layer\n",
    "            kernal_size (int): Size of sliding window for convolution\n",
    "            n_filters (int): Number of filters to produce from convolution\n",
    "        \"\"\"\n",
    "        # Load embedding layer\n",
    "        print('Creating embedding layer....')\n",
    "        word_vectors = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)\n",
    "        \n",
    "        embedding_matrix = np.zeros((len(tokenizer.tokenizer.word_index) + 1, embedding_dim))\n",
    "        for word, i in tokenizer.tokenizer.word_index.items():\n",
    "            try:\n",
    "                embedding_vector = word_vectors[word]\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            except KeyError:\n",
    "                embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),embedding_dim)\n",
    "                \n",
    "        embedding_layer = Embedding(len(tokenizer.tokenizer.word_index) + 1,\n",
    "                                    embedding_dim,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=tokenizer.max_sequence_length,\n",
    "                                    trainable=False)\n",
    "        \n",
    "#         embedding_layer = Embedding(len(tokenizer.tokenizer.word_index) + 1,\n",
    "#                                     embedding_dim,\n",
    "#                                     input_length=tokenizer.max_sequence_length,\n",
    "#                                     trainable=False)\n",
    "\n",
    "        # Create network\n",
    "        print('Creating network...')\n",
    "        sequence_input = Input(shape=(tokenizer.max_sequence_length,), dtype='int32')\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "        \n",
    "        reshape = Reshape((tokenizer.max_sequence_length,embedding_dim,1))(embedded_sequences)\n",
    "\n",
    "        conv_0 = Conv2D(n_filters, (3, embedding_dim),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "        conv_1 = Conv2D(n_filters, (4, embedding_dim),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "        conv_2 = Conv2D(n_filters, (5, embedding_dim),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "        maxpool_0 = MaxPooling2D((tokenizer.max_sequence_length - 3 + 1, 1), strides=(1,1))(conv_0)\n",
    "        maxpool_1 = MaxPooling2D((tokenizer.max_sequence_length - 4 + 1, 1), strides=(1,1))(conv_1)\n",
    "        maxpool_2 = MaxPooling2D((tokenizer.max_sequence_length - 5 + 1, 1), strides=(1,1))(conv_2)\n",
    "\n",
    "        merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "        flatten = Flatten()(merged_tensor)\n",
    "        dropout = Dropout(dropout_fraction)(flatten)\n",
    "        preds = Dense(len(self.category_map), activation='softmax')(dropout)\n",
    "        \n",
    "#         x = Dropout(dropout_fraction)(embedded_sequences)\n",
    "#         x = Conv1D(n_filters, kernal_size, activation='relu')(x)\n",
    "#         x = MaxPooling1D(kernal_size)(x)\n",
    "#         x = LSTM(embedding_dim, dropout=dropout_fraction, recurrent_dropout=dropout_fraction)(x)\n",
    "#         preds = Dense(len(self.category_map), activation='softmax')(x)\n",
    "\n",
    "        # Compile model\n",
    "        print('Compiling network...')\n",
    "        self.model = Model(sequence_input, preds)\n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "                           optimizer='rmsprop',\n",
    "                           metrics=['acc'])\n",
    "\n",
    "    def train(self, data, labels, validation_split=0.25, batch_size=32, epochs=35):\n",
    "        \"\"\"Train classifier\n",
    "        Args:\n",
    "            data (np.array): 3D numpy array (n_samples, embedding_dim, tokenizer.max_sequence_length)\n",
    "            labels (np.array): 2D numpy array (n_samples, len(self.category_map))\n",
    "            validation_split (float): Fraction of samples to be used for validation\n",
    "            batch_size (int): Training batch size\n",
    "            epochs (int): Number of training epochs\n",
    "        \"\"\"\n",
    "        print('Training...')\n",
    "        # Split the data into a training set and a validation set\n",
    "        indices = np.arange(data.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        data = data[indices]\n",
    "        labels = labels[indices]\n",
    "        nb_validation_samples = int(validation_split * data.shape[0])\n",
    "\n",
    "        x_train = data[:-nb_validation_samples]\n",
    "        y_train = labels[:-nb_validation_samples]\n",
    "        x_val = data[-nb_validation_samples:]\n",
    "        y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "        # Train!\n",
    "        self.model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "                       epochs=epochs, batch_size=batch_size)\n",
    "        self.evaluate(x_val, y_val, batch_size)\n",
    "\n",
    "    def evaluate(self, x_test, y_test, batch_size=256):\n",
    "        \"\"\"Evaluate classifier\n",
    "        Args:\n",
    "            x_test (np.array): 3D numpy array (n_samples, embedding_dim, tokenizer.max_sequence_length)\n",
    "            y_test (np.array): 2D numpy array (n_samples, len(self.category_map))\n",
    "            batch_size (int): Training batch size\n",
    "        \"\"\"\n",
    "        print('Evaluating...')\n",
    "        predictions_last_epoch = self.model.predict(x_test, batch_size=batch_size, verbose=1)\n",
    "        predicted_classes = np.argmax(predictions_last_epoch, axis=1)\n",
    "        target_names = ['']*len(self.category_map)\n",
    "        for category in self.category_map:\n",
    "            target_names[self.category_map[category]] = category\n",
    "        y_val = np.argmax(y_test, axis=1)\n",
    "        print(classification_report(y_val, predicted_classes, target_names=target_names, digits = 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get labels from classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing categories...\n",
      "Found 63 unique categories.\n"
     ]
    }
   ],
   "source": [
    "classifier = ProductClassifier()\n",
    "training_labels = classifier.get_labels(training_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile classifier network and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embedding layer....\n",
      "Creating network...\n",
      "Compiling network...\n",
      "Training...\n",
      "Train on 4526 samples, validate on 1508 samples\n",
      "Epoch 1/35\n",
      "4526/4526 [==============================] - 74s 16ms/step - loss: 3.7038 - acc: 0.3294 - val_loss: 3.2208 - val_acc: 0.5597\n",
      "Epoch 2/35\n",
      "4526/4526 [==============================] - 73s 16ms/step - loss: 2.9419 - acc: 0.6001 - val_loss: 2.8773 - val_acc: 0.6432\n",
      "Epoch 3/35\n",
      "4526/4526 [==============================] - 72s 16ms/step - loss: 2.5582 - acc: 0.7112 - val_loss: 2.6493 - val_acc: 0.7162\n",
      "Epoch 4/35\n",
      "4526/4526 [==============================] - 73s 16ms/step - loss: 2.3195 - acc: 0.7793 - val_loss: 2.5158 - val_acc: 0.7248\n",
      "Epoch 5/35\n",
      "4526/4526 [==============================] - 73s 16ms/step - loss: 2.1408 - acc: 0.8084 - val_loss: 2.3523 - val_acc: 0.7586\n",
      "Epoch 6/35\n",
      "4526/4526 [==============================] - 73s 16ms/step - loss: 1.9787 - acc: 0.8445 - val_loss: 2.2434 - val_acc: 0.7699\n",
      "Epoch 7/35\n",
      "4526/4526 [==============================] - 72s 16ms/step - loss: 1.8494 - acc: 0.8562 - val_loss: 2.1474 - val_acc: 0.7792\n",
      "Epoch 8/35\n",
      "4526/4526 [==============================] - 71s 16ms/step - loss: 1.7630 - acc: 0.8685 - val_loss: 2.0642 - val_acc: 0.7958\n",
      "Epoch 9/35\n",
      "4526/4526 [==============================] - 68s 15ms/step - loss: 1.6652 - acc: 0.8917 - val_loss: 2.0235 - val_acc: 0.7851\n",
      "Epoch 10/35\n",
      "4526/4526 [==============================] - 71s 16ms/step - loss: 1.5983 - acc: 0.8844 - val_loss: 1.9708 - val_acc: 0.7905\n",
      "Epoch 11/35\n",
      "4526/4526 [==============================] - 73s 16ms/step - loss: 1.5417 - acc: 0.8992 - val_loss: 1.9194 - val_acc: 0.8037\n",
      "Epoch 12/35\n",
      "4526/4526 [==============================] - 72s 16ms/step - loss: 1.4769 - acc: 0.9121 - val_loss: 1.8777 - val_acc: 0.8057\n",
      "Epoch 13/35\n",
      "4526/4526 [==============================] - 72s 16ms/step - loss: 1.4348 - acc: 0.9158 - val_loss: 1.8291 - val_acc: 0.8143\n",
      "Epoch 14/35\n",
      "4526/4526 [==============================] - 70s 15ms/step - loss: 1.3847 - acc: 0.9187 - val_loss: 1.7924 - val_acc: 0.8057\n",
      "Epoch 15/35\n",
      "4526/4526 [==============================] - 72s 16ms/step - loss: 1.3591 - acc: 0.9171 - val_loss: 1.7721 - val_acc: 0.8137\n",
      "Epoch 16/35\n",
      "4526/4526 [==============================] - 72s 16ms/step - loss: 1.3314 - acc: 0.9196 - val_loss: 1.7485 - val_acc: 0.8084\n",
      "Epoch 17/35\n",
      "4526/4526 [==============================] - 72s 16ms/step - loss: 1.2989 - acc: 0.9227 - val_loss: 1.7395 - val_acc: 0.8183\n",
      "Epoch 18/35\n",
      "4526/4526 [==============================] - 73s 16ms/step - loss: 1.2702 - acc: 0.9286 - val_loss: 1.7252 - val_acc: 0.8156\n",
      "Epoch 19/35\n",
      "4526/4526 [==============================] - 72s 16ms/step - loss: 1.2588 - acc: 0.9222 - val_loss: 1.6957 - val_acc: 0.8123\n",
      "Epoch 20/35\n",
      "4526/4526 [==============================] - 72s 16ms/step - loss: 1.2450 - acc: 0.9266 - val_loss: 1.6879 - val_acc: 0.8130\n",
      "Epoch 21/35\n",
      "4526/4526 [==============================] - 72s 16ms/step - loss: 1.2234 - acc: 0.9280 - val_loss: 1.6836 - val_acc: 0.8156\n",
      "Epoch 22/35\n",
      "4526/4526 [==============================] - 72s 16ms/step - loss: 1.1983 - acc: 0.9331 - val_loss: 1.6749 - val_acc: 0.8103\n",
      "Epoch 23/35\n",
      "4526/4526 [==============================] - 76s 17ms/step - loss: 1.1787 - acc: 0.9326 - val_loss: 1.6749 - val_acc: 0.8137\n",
      "Epoch 24/35\n",
      "4526/4526 [==============================] - 78s 17ms/step - loss: 1.1772 - acc: 0.9331 - val_loss: 1.6683 - val_acc: 0.8130\n",
      "Epoch 25/35\n",
      "4526/4526 [==============================] - 77s 17ms/step - loss: 1.1557 - acc: 0.9368 - val_loss: 1.6374 - val_acc: 0.8143\n",
      "Epoch 26/35\n",
      "4526/4526 [==============================] - 78s 17ms/step - loss: 1.1617 - acc: 0.9311 - val_loss: 1.6383 - val_acc: 0.8196\n",
      "Epoch 27/35\n",
      "4526/4526 [==============================] - 77s 17ms/step - loss: 1.1623 - acc: 0.9302 - val_loss: 1.6104 - val_acc: 0.8223\n",
      "Epoch 28/35\n",
      "4526/4526 [==============================] - 71s 16ms/step - loss: 1.1400 - acc: 0.9333 - val_loss: 1.6171 - val_acc: 0.8150\n",
      "Epoch 29/35\n",
      "4526/4526 [==============================] - 70s 16ms/step - loss: 1.1367 - acc: 0.9328 - val_loss: 1.6384 - val_acc: 0.8137\n",
      "Epoch 30/35\n",
      "4526/4526 [==============================] - 73s 16ms/step - loss: 1.1230 - acc: 0.9359 - val_loss: 1.6169 - val_acc: 0.8117\n",
      "Epoch 31/35\n",
      "4526/4526 [==============================] - 72s 16ms/step - loss: 1.1181 - acc: 0.9366 - val_loss: 1.5826 - val_acc: 0.8243\n",
      "Epoch 32/35\n",
      "4526/4526 [==============================] - 72s 16ms/step - loss: 1.1073 - acc: 0.9370 - val_loss: 1.6187 - val_acc: 0.8163\n",
      "Epoch 33/35\n",
      "4526/4526 [==============================] - 71s 16ms/step - loss: 1.1150 - acc: 0.9390 - val_loss: 1.5973 - val_acc: 0.8143\n",
      "Epoch 34/35\n",
      "4526/4526 [==============================] - 71s 16ms/step - loss: 1.1047 - acc: 0.9355 - val_loss: 1.6057 - val_acc: 0.8090\n",
      "Epoch 35/35\n",
      "4526/4526 [==============================] - 72s 16ms/step - loss: 1.1022 - acc: 0.9370 - val_loss: 1.6016 - val_acc: 0.8143\n",
      "Evaluating...\n",
      "1508/1508 [==============================] - 10s 6ms/step\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                 headphones   0.730769  1.000000  0.844444        19\n",
      "                     cables   0.562500  0.818182  0.666667        22\n",
      "    security & surveillance   0.720000  0.947368  0.818182        19\n",
      "            streaming media   0.793103  1.000000  0.884615        23\n",
      "     television accessories   0.708333  0.629630  0.666667        27\n",
      "             monitor risers   0.730769  0.904762  0.808511        21\n",
      "         gaming accessories   0.636364  0.318182  0.424242        22\n",
      "                video games   0.840000  0.807692  0.823529        26\n",
      "              video cameras   0.850000  0.944444  0.894737        18\n",
      "     3d printers & supplies   0.941176  0.941176  0.941176        17\n",
      "                     drones   0.944444  0.894737  0.918919        19\n",
      "                       mice   0.827586  1.000000  0.905660        24\n",
      "       computer accessories   0.344828  0.434783  0.384615        23\n",
      "                  keyboards   0.961538  0.862069  0.909091        29\n",
      "             monitor mounts   0.777778  0.700000  0.736842        30\n",
      "                   monitors   0.823529  0.875000  0.848485        16\n",
      "         office electronics   0.809524  0.566667  0.666667        30\n",
      "         camera accessories   0.571429  0.307692  0.400000        26\n",
      "            range extenders   0.909091  0.909091  0.909091        22\n",
      "                ink & toner   0.960000  1.000000  0.979592        24\n",
      "  car & vehicle electronics   0.600000  0.473684  0.529412        19\n",
      "           video projectors   1.000000  0.954545  0.976744        22\n",
      "         tablet accessories   0.583333  0.777778  0.666667        27\n",
      "car subwoofers & amplifiers   0.913043  1.000000  0.954545        21\n",
      "                    tablets   0.833333  0.833333  0.833333        24\n",
      "         laptop accessories   0.700000  0.736842  0.717949        19\n",
      "         tripods & monopods   1.000000  0.909091  0.952381        22\n",
      "                televisions   0.880000  0.916667  0.897959        24\n",
      "                  batteries   0.892857  0.925926  0.909091        27\n",
      "                   desktops   0.857143  0.750000  0.800000        24\n",
      "                    laptops   0.814815  0.880000  0.846154        25\n",
      "                 home audio   0.772727  0.566667  0.653846        30\n",
      "           gps & navigation   1.000000  0.833333  0.909091        24\n",
      "            radar detectors   0.960000  1.000000  0.979592        24\n",
      "   mobile phone accessories   0.583333  0.608696  0.595745        23\n",
      "                   headsets   0.904762  0.863636  0.883721        22\n",
      "        binoculars & scopes   1.000000  0.735294  0.847458        34\n",
      "                     modems   0.931034  0.900000  0.915254        30\n",
      "  cases & screen protectors   0.944444  0.772727  0.850000        22\n",
      "         tv mounts & stands   0.800000  0.960000  0.872727        25\n",
      "ebook readers & accessories   0.941176  0.761905  0.842105        21\n",
      "      computer data storage   0.851852  0.793103  0.821429        29\n",
      "  portable audio & speakers   0.666667  0.666667  0.666667        21\n",
      "           power management   1.000000  0.888889  0.941176        18\n",
      "        computer components   0.818182  0.300000  0.439024        30\n",
      "                video cards   0.961538  1.000000  0.980392        25\n",
      "        printers & scanners   0.818182  0.964286  0.885246        28\n",
      "memory cards & flash drives   0.846154  0.758621  0.800000        29\n",
      "       unlocked cell phones   0.807692  0.954545  0.875000        22\n",
      "        wearable technology   0.720000  0.900000  0.800000        20\n",
      "               motherboards   0.807692  0.913043  0.857143        23\n",
      "                 telescopes   1.000000  1.000000  1.000000        23\n",
      "       routers & networking   0.766667  0.696970  0.730159        33\n",
      "              car dash cams   0.787879  1.000000  0.881356        26\n",
      "microphones and accessories   0.814815  0.916667  0.862745        24\n",
      "             two way radios   0.857143  0.888889  0.872727        27\n",
      "    blu-ray and dvd players   0.705882  0.750000  0.727273        16\n",
      "             standing desks   0.916667  0.846154  0.880000        26\n",
      "                    cameras   0.880000  0.880000  0.880000        25\n",
      "                   switches   0.750000  0.857143  0.800000        21\n",
      "                calculators   1.000000  1.000000  1.000000        24\n",
      "              camera lenses   0.862069  0.961538  0.909091        26\n",
      "              game consoles   0.606061  0.769231  0.677966        26\n",
      "\n",
      "                avg / total   0.820908  0.814324  0.808939      1508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier.compile(tokenizer)\n",
    "classifier.train(tokenized_data, training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57030\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TESTING_FILE = \"Data To Classify_Assessment.xlsx\"\n",
    "testing_sheets = pd.ExcelFile(TESTING_FILE)\n",
    "testing_data_df = testing_sheets.parse(\"sample_data\")\n",
    "testing_texts = []\n",
    "for index, row in testing_data_df.iterrows():\n",
    "    if(row[\"BrandName\"]==\"\" or row[\"BrandName\"]==\"NULL\"):\n",
    "        text = row[\"Title\"]\n",
    "    else:\n",
    "        try:\n",
    "            text = row[\"BrandName\"] + \" \" + row[\"Title\"]\n",
    "        except TypeError:\n",
    "            text = str(row[\"BrandName\"]) + \" \" + row[\"Title\"]\n",
    "    testing_texts.append(text)\n",
    "    \n",
    "print(len(testing_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting category index of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57030/57030 [==============================] - 362s 6ms/step\n",
      "(57030,)\n"
     ]
    }
   ],
   "source": [
    "predictions_test_epoch = classifier.model.predict(tokenizer.tokenize(testing_texts), batch_size=32, verbose=1)\n",
    "predicted_test_classes = np.argmax(predictions_test_epoch, axis=1)\n",
    "print(predicted_test_classes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of category names based on index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_names = ['']*len(classifier.category_map)\n",
    "for category in classifier.category_map:\n",
    "    target_names[classifier.category_map[category]] = category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store predicted category in output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "  \n",
    "# Create a workbook and add a worksheet.\n",
    "workbook = xlsxwriter.Workbook('output.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "# Add a bold format to use to highlight cells.\n",
    "bold = workbook.add_format({'bold': 1})\n",
    "\n",
    "# Write some data headers.\n",
    "worksheet.write('A1', 'ASIN', bold)\n",
    "worksheet.write('B1', 'BrandName', bold)\n",
    "worksheet.write('C1', 'Title', bold)\n",
    "worksheet.write('D1', 'Category', bold)\n",
    "\n",
    "row_index = 1\n",
    "col_index = 0\n",
    "\n",
    "for index, row in testing_data_df.iterrows():\n",
    "    worksheet.write(row_index,col_index,row[\"ASIN\"])\n",
    "    try:\n",
    "        if(row[\"BrandName\"]==\"\" or row[\"BrandName\"]==\"NULL\"):\n",
    "            worksheet.write(row_index,col_index+1,\"\")\n",
    "        else:\n",
    "            worksheet.write(row_index,col_index+1,row[\"BrandName\"])\n",
    "    except TypeError:\n",
    "        worksheet.write(row_index,col_index+1,str(row[\"BrandName\"]))\n",
    "    worksheet.write(row_index,col_index+2,row[\"Title\"])\n",
    "    worksheet.write(row_index,col_index+3,target_names[predicted_test_classes[row_index-1]])\n",
    "    row_index += 1\n",
    "    \n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
